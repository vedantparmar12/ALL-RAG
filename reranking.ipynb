{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58608feb-7ce4-40af-96aa-a3c92854fadb",
   "metadata": {},
   "source": [
    "# Reranking\n",
    "\n",
    "<img src=\"./media/reranking.png\" width=600>\n",
    "\n",
    "[Image: Rerankers and Two-Stage Retrieval](https://www.pinecone.io/learn/series/rag/rerankers/)\n",
    "\n",
    "As semantic similarity becomes a core technique for delivering context to LLM-based applications, the challenge of **finding truly relevant information** grows more important. Most modern systems use **embedding models** to convert unstructured text into vector representations, storing these in a vector database for fast similarity-based retrieval.\n",
    "\n",
    "While this first-step retrieval process is efficient and scalable, **the top results may not always be the best-aligned passages for a given query.** They might be ‚Äúnear matches,‚Äù but not the most contextually relevant.\n",
    "This is where **reranking** comes in: a second-stage process that reorders the initially retrieved set to better match the true information need.\n",
    "\n",
    "Reranking uses a dedicated model‚Äîtypically a **cross-encoder** or a **late interaction model** to directly compare each candidate passage with the query, assigning a fine-grained relevance score. By re-evaluating these candidate passages, rerankers help surface  the most useful, specific, and accurate results to the top.\n",
    "\n",
    "In this notebook, we‚Äôll explore the most popular reranking approaches in modern RAG pipelines, with an intuitive look at how these models work and how they improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ed77f-ef82-4bd1-9696-2388504d8e5d",
   "metadata": {},
   "source": [
    "---\n",
    "## Reranking Models\n",
    "\n",
    "<img src=\"./media/embedding.png\" width=600>\n",
    "\n",
    "[Choosing the Right Embedding Model for RAG in Generative AI](https://medium.com/bright-ai/choosing-the-right-embedding-for-rag-in-generative-ai-applications-8cf5b36472e1)\n",
    "\n",
    "In a two stage RAG pipeline, we rely on a few different pre-trained encoder models to convert our unstructured content (generally text) into dense vector representations that capture the learned semantics of language through scaled machine learning. The first of which is the commonly known \"Embedding Model\" set up as a bi-encoder, one for the query one for the document(s). As this notebook is meant to focus on reranking, I will just provide a brief overview of base embedding models in this context, but if you'd like to learn the specifics you can check out [my guide on bidirectional encoder representations from transformers](https://www.youtube.com/watch?v=n_UQ0e0fBIA)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc3c3d-818c-4197-8af1-e00443244879",
   "metadata": {},
   "source": [
    "### Context: Bi-Encoder (Embedding Model)\n",
    "\n",
    "<img src=\"./media/biencoder.png\" width=300>\n",
    "\n",
    "[Cross-encoders vs Bi-encoders : A deep-dive into text encoding methods](https://medium.com/@rbhatia46/cross-encoders-vs-bi-encoders-a-deep-dive-into-text-encoding-methods-d9aa890d6ca4)\n",
    "\n",
    "The **bi-encoder architecture** is widely used for vector similarity search in retrieval systems, where the base embedding model independently encodes queries and documents into numerical vectors (‚Äúembeddings‚Äù) that capture their meaning. At search time, these embeddings are quickly compared (usually via cosine similarity) to find the most relevant matches. This dual encoder architecture is where the \"bi\" from bi-encoder comes from, and typically uses models like [BERT](https://arxiv.org/pdf/1810.04805), which leverage the transformer architecture and attention mechanism to learn powerful language representations via objectives like masked language modeling (MLM). In MLM, the model is trained to predict missing words in sentences, allowing it to develop a deep understanding of context and semantics after training on many millions of examples, allowing the model to perform this dense vector encoding.\n",
    "\n",
    "As a quick clarification, the ‚Äúbi‚Äù in bi-encoder refers to using **two separate encoding passes** (one for queries, one for documents), often with shared model weights which is separate from BERT‚Äôs ‚Äúbidirectional‚Äù attention, which enables each token to attend to both left and right context during training.\n",
    "\n",
    "Let's take a quick look at what this looks like with a popular embedding model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7c1d7-abd8-4493-b512-9c26f272b18b",
   "metadata": {},
   "source": [
    "#### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "155ef125-e07f-4d7d-8481-25d514dd7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f149112-f86e-4c80-a429-6b220a777230",
   "metadata": {},
   "source": [
    "#### Define the Query & Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecfb269-519a-4ed6-9027-11d1b7e2f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the best technical AI youtuber?\"\n",
    "\n",
    "documents = [\n",
    "    \"The dogwood is the state flower of North Carolina\",\n",
    "    \"Adam Lucek makes videos about artificial intelligence\",\n",
    "    \"The canada goose has a lifespan of 10-24 years\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e3bc4-5f58-4534-9e94-70f4c767245b",
   "metadata": {},
   "source": [
    "#### Example Encoding\n",
    "\n",
    "Here we can see what happens when the text is encoded into a numerical form. In the case of the embedding model we're using, our sentence will be mapped to a 384 dimensional dense vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ed96c04-7b25-4ae6-b975-ced15366cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 dimensions:  [-0.08816597 -0.12000839 -0.03506935 -0.10491663  0.01010485 -0.007575\n",
      "  0.08377688  0.0845268   0.01181077 -0.02637845]\n",
      "\n",
      " Total Size:  384 dimensions.\n"
     ]
    }
   ],
   "source": [
    "query_embedding = embedding_model.encode(query)\n",
    "\n",
    "print(\"First 10 dimensions: \", query_embedding[:10])\n",
    "print(\"\\n Total Size: \",len(query_embedding), \"dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7af37d-d789-4394-a409-5371a5d7567d",
   "metadata": {},
   "source": [
    "#### Embedding Documents Independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6081d146-9911-409e-bbb6-aa067fbc9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = embedding_model.encode(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db13a63c-b4d1-47b3-904c-07337b5a4592",
   "metadata": {},
   "source": [
    "#### Computing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b9e465c-fcec-4d75-9c66-aac0dacf4a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0982,  0.6025, -0.0030]])\n"
     ]
    }
   ],
   "source": [
    "similarity = embedding_model.similarity(query_embedding, doc_embeddings)\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe8a6c-2067-4c35-ada0-6a0209e8aa6d",
   "metadata": {},
   "source": [
    "We can see that our query: *Who is the best technical AI youtuber?* is most similar to the embedding in position 2 (index 1): *Adam Lucek makes videos about artificial intelligence* üòé\n",
    "\n",
    "This is the backbone of bi-encoder based vector similarity based retrieval, converting and storing document embeddings then comparing them at run time to embedded queries. This done a lot more efficiently than our quick example using [vector databases](https://github.com/ALucek/embeddings-guide/blob/main/WTF_VDB.ipynb), but supports our first stage of the two stage RAG process- initial retrieval. Once our retrieved set is available, we can then employ our reranking models to provide a more refined ranking of relevant context for query responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199dd7f7-e440-4d4a-b38a-f32b3f089b7e",
   "metadata": {},
   "source": [
    "### Cross Encoder\n",
    "\n",
    "<img src=\"./media/cross-encoder.png\" width=800>\n",
    "\n",
    "[The Illustrated Guide to Cross-Encoders: From Deep to Shallow](https://medium.com/@kakumar1611/the-illustrated-guide-to-cross-encoders-from-deep-to-shallow-2a23a8630016)\n",
    "\n",
    "The **cross-encoder architecture** extends base embedding models by training them as classifiers for direct semantic similarity or relevance. Rather than computing document and query embeddings independently for later comparison, cross-encoders take the query and each document and **concatenate them as a single input** (e.g., `[CLS] query [SEP] document [SEP]`) to the model. The output is a **direct relevance score** for the query-document pair, typically produced by the model‚Äôs classification \\[CLS] token (or an output head attached to it).\n",
    "\n",
    "The key advantage of the cross-encoder is that **every token in the input (query and document) can attend to every other token**, allowing for richer, fine-grained interactions between query and document. This usually yields higher accuracy and more nuanced matching than bi-encoders. However, this comes at a computational cost as each query-document pair must be processed together, which is slower than similarity search with precomputed embeddings, hence why it's often applied as the second step in retrieval after a smaller candidate set has been retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1149365-e9b1-4709-af1e-565cba4a8b34",
   "metadata": {},
   "source": [
    "#### Load the Model\n",
    "\n",
    "For this example we'll be using the popular cross-encoder [ms-marco-MiniLM-L6-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44daa3d4-742f-4071-9cfa-2391761bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bcf2b2-00d5-4264-a8d3-1a8f34e5e92e",
   "metadata": {},
   "source": [
    "#### Define the Query & Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cfd2514-4f62-4386-a0d5-06776a6c82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the health benefits of meditation?\"\n",
    "\n",
    "documents = [\n",
    "    \"Several clinical studies have shown that regular meditation can help reduce stress and anxiety levels.\",\n",
    "    \"Meditation involves focusing the mind and eliminating distractions, often through breathing techniques or guided imagery.\",\n",
    "    \"A daily meditation practice has been associated with lower blood pressure and improved sleep quality in adults.\",\n",
    "    \"The city of Kyoto is famous for its Zen temples, where meditation has been practiced for centuries.\",\n",
    "    \"People who meditate frequently often report feeling calmer and more focused throughout the day.\",\n",
    "    \"Research suggests meditation may lower the risk of heart disease by reducing inflammation and improving heart rate variability.\",\n",
    "    \"Meditation apps have become increasingly popular, offering guided sessions on mindfulness and relaxation.\",\n",
    "    \"A 2021 meta-analysis found that meditation can reduce symptoms of depression when used alongside other treatments.\",\n",
    "    \"Some forms of meditation emphasize compassion and kindness, aiming to improve emotional well-being.\",\n",
    "    \"Athletes sometimes use meditation techniques to enhance concentration and mental resilience during competition.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239d1c6-6213-451b-9d8c-f5638596385f",
   "metadata": {},
   "source": [
    "#### Rank Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90e486c5-e464-4b00-9cab-5244d4667e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Cross Encoder Rankings ========================= \n",
      "\n",
      "7.99\tResearch suggests meditation may lower the risk of heart disease by reducing inflammation and improving heart rate variability.\n",
      "6.95\tSeveral clinical studies have shown that regular meditation can help reduce stress and anxiety levels.\n",
      "6.31\tA 2021 meta-analysis found that meditation can reduce symptoms of depression when used alongside other treatments.\n",
      "5.87\tA daily meditation practice has been associated with lower blood pressure and improved sleep quality in adults.\n",
      "2.54\tSome forms of meditation emphasize compassion and kindness, aiming to improve emotional well-being.\n",
      "1.02\tMeditation involves focusing the mind and eliminating distractions, often through breathing techniques or guided imagery.\n",
      "0.47\tAthletes sometimes use meditation techniques to enhance concentration and mental resilience during competition.\n",
      "-2.84\tPeople who meditate frequently often report feeling calmer and more focused throughout the day.\n",
      "-2.97\tMeditation apps have become increasingly popular, offering guided sessions on mindfulness and relaxation.\n",
      "-9.37\tThe city of Kyoto is famous for its Zen temples, where meditation has been practiced for centuries.\n"
     ]
    }
   ],
   "source": [
    "ranks = cross_encoder.rank(query, documents)\n",
    "\n",
    "print(\"=\"*25, \"Cross Encoder Rankings\", \"=\"*25, \"\\n\")\n",
    "for rank in ranks:\n",
    "    print(f\"{rank['score']:.2f}\\t{documents[rank['corpus_id']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700a0be-eb34-41ff-81ad-ece0320fda55",
   "metadata": {},
   "source": [
    "As you can see we're able to order the passages a lot more closer to what's relevant to our query! Whereas a regular bi-encoder retrieval may focus too much on keywords and themes we're able to clearly extract the specifics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d1973-a0a2-401e-9648-78f77d84470c",
   "metadata": {},
   "source": [
    "### Late Interaction Model\n",
    "\n",
    "<img src=\"./media/multi.png\" width=800>\n",
    "\n",
    "[ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/pdf/2004.12832)\n",
    "\n",
    "The late interaction architecture came after the popularization of using a cross-encoder with the release of ColBERT research and introduces a unique approach inspired by both the bi-encoder and cross-encoder setup. Late interaction based reranking takes the set of retrieved documents and query and splits each into their respective tokens. The query and each candidate document are then encoded as a matrix of token level vectors. This is opposed to the pooled single vector representation that is output by a base embedding model that combines all the token level embeddings together. Then for each query token the similarity is compared to all document token vectors and the maximum value is kept. This maximum similarity is aggregated across all query tokens to produce the final relevancy score for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1086f-ac6b-4964-88e3-1f52023ad716",
   "metadata": {},
   "source": [
    "<img src=\"./media/late-interaction.excalidraw.png\" width=800>\n",
    "\n",
    "Mathematically, given:\n",
    "\n",
    "- Query $Q$ with tokens $q_1, q_2, \\ldots, q_m$\n",
    "- Candidate document $D$ with tokens $d_1, d_2, \\ldots, d_n$\n",
    "\n",
    "Each token is encoded into a vector: $Q = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_m]$, $D = [\\mathbf{d}_1, \\mathbf{d}_2, \\ldots, \\mathbf{d}_n]$.\n",
    "\n",
    "For each query token $\\mathbf{q}_j$: $s_j = \\max_k \\left( \\cos (\\mathbf{q}_j, \\mathbf{d}_k) \\right)$\n",
    "\n",
    "The final document relevance score $S$ is aggregated by sum or mean:\n",
    "$S = \\sum_{j=1}^m s_j$, or $S = \\frac{1}{m} \\sum_{j=1}^m s_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01f5fd-aa20-4ee0-8f94-027562821014",
   "metadata": {},
   "source": [
    "#### Ranking with ColBERT\n",
    "\n",
    "We'll be using [ColBERT V2](https://huggingface.co/colbert-ir/colbertv2.0) for our demonstration. Along with the model is an [official repo](https://github.com/stanford-futuredata/ColBERT/tree/main) that includes specific abstractions and functions meant to process documents and ranking with ColBERT and the proposed late interaction approach. But for demonstrations sake we'll implement late interaction directly with `transformers` and `torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627d26f-4689-40cd-aaf2-c7b95dbb157e",
   "metadata": {},
   "source": [
    "#### Load the Model\n",
    "\n",
    "We'll use the [transformers](https://huggingface.co/docs/transformers/en/index) package directly to load the tokenizer and model from the ü§ó Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3577ae2b-8dd5-46f0-b027-7a29dd373551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541d4df-5d14-4c25-846c-75fa38c6a6c8",
   "metadata": {},
   "source": [
    "#### Define the Query & Documents\n",
    "\n",
    "Same from before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffb40512-7d5d-47e0-bbc7-348a90921206",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the health benefits of meditation?\"\n",
    "\n",
    "documents = [\n",
    "    \"Several clinical studies have shown that regular meditation can help reduce stress and anxiety levels.\",\n",
    "    \"Meditation involves focusing the mind and eliminating distractions, often through breathing techniques or guided imagery.\",\n",
    "    \"A daily meditation practice has been associated with lower blood pressure and improved sleep quality in adults.\",\n",
    "    \"The city of Kyoto is famous for its Zen temples, where meditation has been practiced for centuries.\",\n",
    "    \"People who meditate frequently often report feeling calmer and more focused throughout the day.\",\n",
    "    \"Research suggests meditation may lower the risk of heart disease by reducing inflammation and improving heart rate variability.\",\n",
    "    \"Meditation apps have become increasingly popular, offering guided sessions on mindfulness and relaxation.\",\n",
    "    \"A 2021 meta-analysis found that meditation can reduce symptoms of depression when used alongside other treatments.\",\n",
    "    \"Some forms of meditation emphasize compassion and kindness, aiming to improve emotional well-being.\",\n",
    "    \"Athletes sometimes use meditation techniques to enhance concentration and mental resilience during competition.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2b7a2-3228-43b3-8a1b-08bef646bf17",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "\n",
    "We'll be defining two helper functions, `get_token_embeddings` and `colbert_score`.\n",
    "\n",
    "`get_token_embeddings` will compute the token-level embeddings through the model, taking in text and first running it through the tokenizer to split into individual tokens. Those tokens are then sent through the model to create a vector representation for each. We remove any special classifier or separator tokens that may have been output and keep just the individual embeddings.\n",
    "\n",
    "`colbert_score` then takes in the query embeddings and document embeddings, which have the shape of matrices:\n",
    "\n",
    "* The **query embeddings** have shape \\$(m, d)\\$, where \\$m\\$ is the number of query tokens and \\$d\\$ is the embedding dimension.\n",
    "* The **document embeddings** have shape \\$(n, d)\\$, where \\$n\\$ is the number of document tokens.\n",
    "\n",
    "The scoring function computes the **cosine similarity** between each query token embedding and every document token embedding, resulting in an \\$(m, n)\\$ similarity matrix. For each query token, we take the **maximum similarity** value across all document tokens. These maximum similarities are then **summed** across all query tokens to produce the final relevance score for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf9d8939-6eb4-429a-994b-0154659e41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_token_embeddings(text, tokenizer, model):\n",
    "    # Get token-level embeddings, ignore [CLS] and [SEP]\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # outputs.last_hidden_state: (1, seq_len, hidden_dim)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
    "    \n",
    "    return outputs.last_hidden_state[0][keep_indices]  # (filtered_seq_len, hidden_dim)\n",
    "\n",
    "def colbert_score(query_emb, doc_emb):\n",
    "    \n",
    "    # query_emb: (m, d), doc_emb: (n, d)\n",
    "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)  # (m, n)\n",
    "    \n",
    "    max_sim, _ = sim.max(dim=1)  # (m,)\n",
    "    \n",
    "    return max_sim.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9d441-48da-4508-81e1-b7ac714e802d",
   "metadata": {},
   "source": [
    "#### Rank Documents\n",
    "\n",
    "Now we put it all together by embedding the query and each document, computing the scores, then sorting the documents by relevance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7759ab5f-ec5d-40cb-9faa-4f085abcd574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Late Interaction Rankings ========================= \n",
      "\n",
      "6.02\tResearch suggests meditation may lower the risk of heart disease by reducing inflammation and improving heart rate variability.\n",
      "5.93\tA daily meditation practice has been associated with lower blood pressure and improved sleep quality in adults.\n",
      "5.88\tSeveral clinical studies have shown that regular meditation can help reduce stress and anxiety levels.\n",
      "5.71\tA 2021 meta-analysis found that meditation can reduce symptoms of depression when used alongside other treatments.\n",
      "5.63\tSome forms of meditation emphasize compassion and kindness, aiming to improve emotional well-being.\n",
      "5.12\tMeditation apps have become increasingly popular, offering guided sessions on mindfulness and relaxation.\n",
      "5.10\tAthletes sometimes use meditation techniques to enhance concentration and mental resilience during competition.\n",
      "4.91\tMeditation involves focusing the mind and eliminating distractions, often through breathing techniques or guided imagery.\n",
      "4.41\tPeople who meditate frequently often report feeling calmer and more focused throughout the day.\n",
      "4.23\tThe city of Kyoto is famous for its Zen temples, where meditation has been practiced for centuries.\n"
     ]
    }
   ],
   "source": [
    "# Compute token level embeddings\n",
    "query_emb = get_token_embeddings(query, colbert_tokenizer, colbert)\n",
    "doc_embs = [get_token_embeddings(doc, colbert_tokenizer, colbert) for doc in documents]\n",
    "\n",
    "# Compute scores\n",
    "scores = [colbert_score(query_emb, doc_emb) for doc_emb in doc_embs]\n",
    "\n",
    "ranking = sorted(zip(scores, documents), reverse=True)\n",
    "\n",
    "print(\"=\"*25, \"Late Interaction Rankings\", \"=\"*25, \"\\n\")\n",
    "for score, doc in ranking:\n",
    "    print(f\"{score:.2f}\\t{doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27417b-f93e-44be-a643-d74b9b513737",
   "metadata": {},
   "source": [
    "<img src=\"./media/colbert_evals.png\" width=800>\n",
    "\n",
    "In the ColBERT research, they found that this late interaction approach provided comparable performance to existing cross encoders and other reranking methods but was quicker and required much less computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443a9b6-d4fd-47c0-89a9-a77037895ae4",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "Now that we have an understanding of the models and approaches for reranking, let's create a simple RAG pipeline that can query and rerank results for a RAG response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26c508-3b61-4f0c-875f-6c7a01bc3204",
   "metadata": {},
   "source": [
    "### Vector Database Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bced03-2844-49e3-9694-d86ed4984c20",
   "metadata": {},
   "source": [
    "#### Text Chunking\n",
    "\n",
    "For our database we'll do a simplified chunking setup on [The Adventures of Sherlock Holmes](https://www.gutenberg.org/ebooks/1661) as available through Project Gutenberg. We'll grab the text from the website and setup a very simple recursive token chunker that first splits the text into sentences, then combines into chunks of roughly 400 tokens long. These will be our candidate chunks for retrieval embedded into our vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da539525-f3c7-4e57-a9c2-a39da838b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 380\n",
      "\n",
      "Sample chunk:\n",
      " ‚ÄúYour Majesty had not spoken before I\n",
      "was aware that I was addressing Wilhelm Gottsreich Sigismond von\n",
      "Ormstein, Grand Duke of Cassel-Felstein, and hereditary King of\n",
      "Bohemia.‚Äù\n",
      "\n",
      "‚ÄúBut you can understand,‚Äù said our strange visitor, sitting down once\n",
      "more and passing his hand over his high white forehead, ‚Äúyou can\n",
      "understand that I am not accustomed to doing such business in my own\n",
      "person. Yet the matter was so delicate that I could not confide it to\n",
      "an agent without putting myself in his power. I have come _incognito_\n",
      "from Prague for the purpose of consulting you.‚Äù\n",
      "\n",
      "‚ÄúThen, pray consult,‚Äù said Holmes, shutting his eyes once more. ‚ÄúThe facts are briefly these: Some five years ago, during a lengthy\n",
      "visit to Warsaw, I made the acquaintance of the well-known adventuress,\n",
      "Irene Adler. The name is no doubt familiar to you.‚Äù\n",
      "\n",
      "‚ÄúKindly look her up in my index, Doctor,‚Äù murmured Holmes without\n",
      "opening his eyes. For many years he had adopted a system of docketing\n",
      "all paragraphs concerning men and things, so that it was difficult to\n",
      "name a subject or a person on which he could not at once furnish\n",
      "information. In this case I found her biography sandwiched in between\n",
      "that of a Hebrew rabbi and that of a staff-commander who had written a\n",
      "monograph upon the deep-sea fishes. ‚ÄúLet me see!‚Äù said Holmes. ‚ÄúHum! Born in New Jersey in the year 1858. Contralto‚Äîhum! La Scala, hum! Prima donna Imperial Opera of Warsaw‚Äîyes! Retired from operatic stage‚Äîha! Living in London‚Äîquite so!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import tiktoken\n",
    "\n",
    "# Download Sherlock Holmes text\n",
    "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Sentence split\n",
    "# Download punkt_tab sentence tokenizer from the natural language toolkit\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Setup GPT-4 tokenizer @ 400 tokens target\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_limit = 400\n",
    "\n",
    "# Initiate chunking index and list\n",
    "chunks = []\n",
    "current_chunk = \"\"\n",
    "current_tokens = 0\n",
    "\n",
    "# Chunk\n",
    "for sentence in sentences:\n",
    "    sentence_tokens = len(enc.encode(sentence))\n",
    "    # If adding this sentence would go over the limit, start a new chunk\n",
    "    if current_tokens + sentence_tokens > token_limit:\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        current_chunk = sentence\n",
    "        current_tokens = sentence_tokens\n",
    "    else:\n",
    "        if current_chunk:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            current_chunk = sentence\n",
    "        current_tokens += sentence_tokens\n",
    "\n",
    "# Don't forget the last chunk!\n",
    "if current_chunk:\n",
    "    chunks.append(current_chunk)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"\\nSample chunk:\\n\", chunks[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07ad81-6b7b-439b-973d-6ab09a66c916",
   "metadata": {},
   "source": [
    "#### VDB Initialization\n",
    "\n",
    "We'll use [ChromaDB](https://www.trychroma.com/) as our lightweight database of choice. ChromaDB by default relies on the embedding model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for embedding documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a64d77-302e-4540-9d8d-29eacfc56582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Instantiate the Chroma Client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a Collection\n",
    "collection = chroma_client.get_or_create_collection(name=\"sherlock_holmes\")\n",
    "\n",
    "# Embed Chunks to the Collection\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=[str(i) for i in range(len(chunks))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ab05d-d7d0-42ed-ad2c-cd22e22c7877",
   "metadata": {},
   "source": [
    "#### 1st Stage Retrieval\n",
    "\n",
    "We'll create a simple function to handle the initial retrieval from the collection, what's performed before reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faecd249-7002-4c80-a727-e7f6b935cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, collection=\"sherlock_holmes\", n=25):\n",
    "    # Load Chroma Collection\n",
    "    collection = chroma_client.get_or_create_collection(name=collection)\n",
    "\n",
    "    # Perform semantic search\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n\n",
    "    )\n",
    "\n",
    "    # Zip documents and distances together into dicts\n",
    "    docs = results[\"documents\"][0]\n",
    "    scores = results[\"distances\"][0]\n",
    "\n",
    "    # Combine into list of dicts\n",
    "    return [{\"document\": doc, \"score\": score} for doc, score in zip(docs, scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024487f-6d14-4bbe-8ae7-0cf774f2d0fb",
   "metadata": {},
   "source": [
    "#### 2nd Stage Reranking - Cross Encoder\n",
    "\n",
    "The first of two reranking functions, running our retrieved through a cross encoder, using the same as in our prior example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30a13433-e1b4-4dc0-864f-a693372852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, results, cross_encoder_model=cross_encoder):\n",
    "    # Grab chunks from dictionary\n",
    "    documents = [r['document'] for r in results]\n",
    "    \n",
    "    # Compute cross encoder relevancy score\n",
    "    rerank_scores = cross_encoder_model.predict([(query, doc) for doc in documents])\n",
    "    \n",
    "    for r, score in zip(results, rerank_scores):\n",
    "        r['cross_encoder_score'] = float(score)\n",
    "    \n",
    "    # Sort results by cross_encoder_score, descending\n",
    "    results = sorted(results, key=lambda x: x['cross_encoder_score'], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26551b7-0a3b-4257-bf67-a7692edfb95f",
   "metadata": {},
   "source": [
    "#### 2nd Stage Reranking - Late Interaction\n",
    "\n",
    "The second of our reranking functions, late interaction using ColBERT once more! We'll redefine our prior two helper functions again if this is being ran in isolation from the walkthrough code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7eacccaa-072b-4b7d-8d3a-9d38e94c802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load ColBERTv2 model & tokenizer just once (at top level)\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "def get_token_embeddings(text, tokenizer, model):\n",
    "    # Get token-level embeddings, ignore [CLS] and [SEP]\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
    "    return outputs.last_hidden_state[0][keep_indices]  # (filtered_seq_len, hidden_dim)\n",
    "\n",
    "def colbert_score(query_emb, doc_emb):\n",
    "    # query_emb: (m, d), doc_emb: (n, d)\n",
    "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)  # (m, n)\n",
    "    max_sim, _ = sim.max(dim=1)  # (m,)\n",
    "    return max_sim.sum().item()\n",
    "\n",
    "def rerank_with_late_interaction(query, results, tokenizer=colbert_tokenizer, model=colbert_model):\n",
    "    # Precompute query embeddings once\n",
    "    query_emb = get_token_embeddings(query, tokenizer, model)\n",
    "    for r in results:\n",
    "        doc_emb = get_token_embeddings(r['document'], tokenizer, model)\n",
    "        r['late_interaction_score'] = colbert_score(query_emb, doc_emb)\n",
    "    # Sort results by late_interaction_score descending\n",
    "    results = sorted(results, key=lambda x: x['late_interaction_score'], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d59024-bd81-4e95-9d94-e2236c1acfc1",
   "metadata": {},
   "source": [
    "### Testing it Out!\n",
    "\n",
    "Let's now run the entire pipeline and compare our outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c86685-4488-422f-8943-34232cda936e",
   "metadata": {},
   "source": [
    "**Stage 1: Retrieval only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45060e33-f742-44ff-821d-302271df3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Top 5 Retrieved (Semantic Search Only) ====================\n",
      "[1] Score: 0.9414\n",
      "‚ÄúIf the police are to\n",
      "hush this thing up, there must be no more of Hugh Boone.‚Äù\n",
      "\n",
      "‚ÄúI have sworn it by the most solemn oaths which a man can take.‚Äù\n",
      "\n",
      "‚ÄúIn that case I think that it is probable that n...\n",
      "\n",
      "[2] Score: 0.9463\n",
      "I simply wish to hear your real, real opinion.‚Äù\n",
      "\n",
      "‚ÄúUpon what point?‚Äù\n",
      "\n",
      "‚ÄúIn your heart of hearts, do you think that Neville is alive?‚Äù\n",
      "\n",
      "Sherlock Holmes seemed to be embarrassed by the question. ‚ÄúFr...\n",
      "\n",
      "[3] Score: 0.9467\n",
      "It may be so in this\n",
      "case, also.‚Äù\n",
      "\n",
      "‚ÄúWell, let us hope so. But our doubts will very soon be solved, for\n",
      "here, unless I am much mistaken, is the person in question.‚Äù\n",
      "\n",
      "As he spoke the door opened a...\n",
      "\n",
      "[4] Score: 0.9587\n",
      "He\n",
      "said that if they were sent to the office he would be chaffed by all\n",
      "the other clerks about having letters from a lady, so I offered to\n",
      "typewrite them, like he did his, but he wouldn‚Äôt have that...\n",
      "\n",
      "[5] Score: 0.9642\n",
      "He wore rather baggy grey shepherd‚Äôs check trousers,\n",
      "a not over-clean black frock-coat, unbuttoned in the front, and a drab\n",
      "waistcoat with a heavy brassy Albert chain, and a square pierced bit of\n",
      "m...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Show moments where Holmes or Watson reflect on friendship.\"\n",
    "\n",
    "# Stage 1: Retrieve top 50\n",
    "retrieved_docs = retrieve_docs(query, collection=\"sherlock_holmes\", n=50)\n",
    "\n",
    "print(\"=\"*20, \"Top 5 Retrieved (Semantic Search Only)\", \"=\"*20)\n",
    "for i, r in enumerate(retrieved_docs[:5]):\n",
    "    print(f\"[{i+1}] Score: {r['score']:.4f}\\n{r['document'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e34396-2f14-41f4-86d8-47e41b83d701",
   "metadata": {},
   "source": [
    "**Stage 2: Cross-Encoder reranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c981482f-b806-4529-8349-1207872d6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Top 5 After Cross Encoder Rerank ====================\n",
      "[1] Cross-Encoder Score: -1.56\n",
      "‚ÄúVery sorry to knock you up, Watson,‚Äù said he, ‚Äúbut it‚Äôs the common lot\n",
      "this morning. Mrs. Hudson has been knocked up, she retorted upon me,\n",
      "and I on you.‚Äù\n",
      "\n",
      "‚ÄúWhat is it, then‚Äîa fire?‚Äù\n",
      "\n",
      "‚ÄúNo; a cl...\n",
      "\n",
      "[2] Cross-Encoder Score: -1.81\n",
      "Holmes unlocked his strong-box and held up the blue\n",
      "carbuncle, which shone out like a star, with a cold, brilliant,\n",
      "many-pointed radiance. Ryder stood glaring with a drawn face, uncertain\n",
      "whether t...\n",
      "\n",
      "[3] Cross-Encoder Score: -2.61\n",
      "A few moments later he was in our room, still puffing, still\n",
      "gesticulating, but with so fixed a look of grief and despair in his\n",
      "eyes that our smiles were turned in an instant to horror and pity. Fo...\n",
      "\n",
      "[4] Cross-Encoder Score: -2.77\n",
      "Holmes cut the cord and removed the transverse bar. Then he\n",
      "tried the various keys in the lock, but without success. No sound came\n",
      "from within, and at the silence Holmes‚Äô face clouded over. ‚ÄúI trust...\n",
      "\n",
      "[5] Cross-Encoder Score: -2.84\n",
      "Sherlock Holmes sat moodily at one side of the\n",
      "fireplace cross-indexing his records of crime, while I at the other was\n",
      "deep in one of Clark Russell‚Äôs fine sea-stories until the howl of the\n",
      "gale fro...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_encoder_reranked = rerank_with_cross_encoder(query, retrieved_docs)\n",
    "\n",
    "print(\"=\"*20, \"Top 5 After Cross Encoder Rerank\", \"=\"*20)\n",
    "for i, r in enumerate(cross_encoder_reranked[:5]):\n",
    "    print(f\"[{i+1}] Cross-Encoder Score: {r['cross_encoder_score']:.2f}\\n{r['document'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf2d26-385f-4ced-bc1c-d02631780bf4",
   "metadata": {},
   "source": [
    "**Stage 3: Late Interaction reranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9aed1590-7b56-456f-8424-97aefae05272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Top 5 After Late Interaction Rerank ====================\n",
      "[1] Late Interaction Score: 6.50\n",
      "This gentleman, Mr. Wilson, has been my partner and helper\n",
      "in many of my most successful cases, and I have no doubt that he will\n",
      "be of the utmost use to me in yours also.‚Äù\n",
      "\n",
      "The stout gentleman hal...\n",
      "\n",
      "[2] Late Interaction Score: 6.44\n",
      "Holmes unlocked his strong-box and held up the blue\n",
      "carbuncle, which shone out like a star, with a cold, brilliant,\n",
      "many-pointed radiance. Ryder stood glaring with a drawn face, uncertain\n",
      "whether t...\n",
      "\n",
      "[3] Late Interaction Score: 6.38\n",
      "THE ADVENTURE OF THE ENGINEER‚ÄôS THUMB\n",
      "\n",
      "\n",
      "Of all the problems which have been submitted to my friend, Mr.\n",
      "Sherlock Holmes, for solution during the years of our intimacy, there\n",
      "were only two which I...\n",
      "\n",
      "[4] Late Interaction Score: 6.30\n",
      "Holmes cut the cord and removed the transverse bar. Then he\n",
      "tried the various keys in the lock, but without success. No sound came\n",
      "from within, and at the silence Holmes‚Äô face clouded over. ‚ÄúI trust...\n",
      "\n",
      "[5] Late Interaction Score: 6.27\n",
      "A few moments later he was in our room, still puffing, still\n",
      "gesticulating, but with so fixed a look of grief and despair in his\n",
      "eyes that our smiles were turned in an instant to horror and pity. Fo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "late_interaction_reranked = rerank_with_late_interaction(query, retrieved_docs)\n",
    "\n",
    "print(\"=\"*20, \"Top 5 After Late Interaction Rerank\", \"=\"*20)\n",
    "for i, r in enumerate(late_interaction_reranked[:5]):\n",
    "    print(f\"[{i+1}] Late Interaction Score: {r['late_interaction_score']:.2f}\\n{r['document'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484eba5e-6232-4b2d-be1b-f59da4bdddbd",
   "metadata": {},
   "source": [
    "### Combining with an LLM for RAG\n",
    "\n",
    "Now that we have our retrieval and reranking systems in place, we can now combine everything into a full fledged RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e24934b-2eb3-49bd-b36c-9719c75611a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def rag_response(query, ranking=\"none\", k=5):\n",
    "\n",
    "    # Instantiate OpenAI Client\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Retrieve our initial set of 50 documents\n",
    "    retrieved_documents = retrieve_docs(query, n=50)  # returns list of dicts with 'document' (and scores)\n",
    "\n",
    "    # Rerank and sort based on our three methods\n",
    "    if ranking == \"none\":\n",
    "        sorted_docs = sorted(retrieved_documents, key=lambda r: r['score'])\n",
    "        docs = [r['document'] for r in sorted_docs[:k]]\n",
    "    elif ranking == \"cross_encoder\":\n",
    "        reranked = rerank_with_cross_encoder(query, retrieved_documents)\n",
    "        docs = [r['document'] for r in reranked[:k]]\n",
    "    elif ranking == \"late_interaction\":\n",
    "        reranked = rerank_with_late_interaction(query, retrieved_documents)\n",
    "        docs = [r['document'] for r in reranked[:k]]\n",
    "    else:\n",
    "        raise ValueError(\"Argument 'ranking' must be one of ['none', 'cross_encoder', 'late_interaction']\") \n",
    "\n",
    "    # DEBUG PRINT: show which chunks are being provided\n",
    "    print(f\"\\nProvided Chunks (top {k}, ranking: {ranking}):\\n\" + \"-\"*60)\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"[{i}] {doc[:250]}{'...' if len(doc)>250 else ''}\\n\")  # Show first 250 chars for readability\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Combine top documents for passage to LLM\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    prompt = f\"\"\"You are a Sherlock Holmes expert. Use ONLY the following passages from the stories to answer the question.\n",
    "\n",
    "Passages:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "If you cannot find an answer in the passages, reply: \"The answer is not shown in the provided context.\" Otherwise, answer as specifically as possible using the text above.\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78994ffb-d0c6-4240-996c-e292adf1ac97",
   "metadata": {},
   "source": [
    "**Simple Semantic Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad4255c7-13c8-409d-9927-f46f134d4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided Chunks (top 5, ranking: none):\n",
      "------------------------------------------------------------\n",
      "[1] ‚ÄúIf the police are to\n",
      "hush this thing up, there must be no more of Hugh Boone.‚Äù\n",
      "\n",
      "‚ÄúI have sworn it by the most solemn oaths which a man can take.‚Äù\n",
      "\n",
      "‚ÄúIn that case I think that it is probable that no further steps may be\n",
      "taken. But if you are foun...\n",
      "\n",
      "[2] I simply wish to hear your real, real opinion.‚Äù\n",
      "\n",
      "‚ÄúUpon what point?‚Äù\n",
      "\n",
      "‚ÄúIn your heart of hearts, do you think that Neville is alive?‚Äù\n",
      "\n",
      "Sherlock Holmes seemed to be embarrassed by the question. ‚ÄúFrankly,\n",
      "now!‚Äù she repeated, standing upon the rug ...\n",
      "\n",
      "[3] It may be so in this\n",
      "case, also.‚Äù\n",
      "\n",
      "‚ÄúWell, let us hope so. But our doubts will very soon be solved, for\n",
      "here, unless I am much mistaken, is the person in question.‚Äù\n",
      "\n",
      "As he spoke the door opened and a young lady entered the room. She was\n",
      "plainly...\n",
      "\n",
      "[4] He\n",
      "said that if they were sent to the office he would be chaffed by all\n",
      "the other clerks about having letters from a lady, so I offered to\n",
      "typewrite them, like he did his, but he wouldn‚Äôt have that, for he said\n",
      "that when I wrote them they seemed ...\n",
      "\n",
      "[5] He wore rather baggy grey shepherd‚Äôs check trousers,\n",
      "a not over-clean black frock-coat, unbuttoned in the front, and a drab\n",
      "waistcoat with a heavy brassy Albert chain, and a square pierced bit of\n",
      "metal dangling down as an ornament. A frayed top-ha...\n",
      "\n",
      "==================================================\n",
      "There are two moments in the provided passages where Holmes or Watson reflect on friendship:\n",
      "\n",
      "1. In the passage from \"THE ADVENTURE OF THE BLUE CARBUNCLE,\" Watson visits Holmes after Christmas:\n",
      "\n",
      ">I had called upon my friend Sherlock Holmes upon the second morning after Christmas, with the intention of wishing him the compliments of the season.\n",
      "\n",
      "This shows Watson reflecting on their friendship by visiting Holmes to wish him well for the season.\n",
      "\n",
      "2. In the same passage, Holmes expresses pleasure at Watson's company:\n",
      "\n",
      ">‚ÄúYou are engaged,‚Äù said I; ‚Äúperhaps I interrupt you.‚Äù\n",
      ">\n",
      ">‚ÄúNot at all. I am glad to have a friend with whom I can discuss my results.\n",
      "\n",
      "Here, Holmes directly refers to Watson as a friend and expresses gladness at having him to discuss his results with.\n",
      "\n",
      "These are moments where friendship is acknowledged or reflected upon in the provided context.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_response(\"Show moments where Holmes or Watson reflect on friendship.\", ranking=\"none\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa441c-972d-4531-a07e-448493cbbb7c",
   "metadata": {},
   "source": [
    "**Reranking with Cross Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "600dd1ee-ca06-4653-9a35-0a0cd6267873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided Chunks (top 5, ranking: cross_encoder):\n",
      "------------------------------------------------------------\n",
      "[1] ‚ÄúVery sorry to knock you up, Watson,‚Äù said he, ‚Äúbut it‚Äôs the common lot\n",
      "this morning. Mrs. Hudson has been knocked up, she retorted upon me,\n",
      "and I on you.‚Äù\n",
      "\n",
      "‚ÄúWhat is it, then‚Äîa fire?‚Äù\n",
      "\n",
      "‚ÄúNo; a client. It seems that a young lady has arrived in a ...\n",
      "\n",
      "[2] Holmes unlocked his strong-box and held up the blue\n",
      "carbuncle, which shone out like a star, with a cold, brilliant,\n",
      "many-pointed radiance. Ryder stood glaring with a drawn face, uncertain\n",
      "whether to claim or to disown it. ‚ÄúThe game‚Äôs up, Ryder,‚Äù s...\n",
      "\n",
      "[3] A few moments later he was in our room, still puffing, still\n",
      "gesticulating, but with so fixed a look of grief and despair in his\n",
      "eyes that our smiles were turned in an instant to horror and pity. For\n",
      "a while he could not get his words out, but swa...\n",
      "\n",
      "[4] Holmes cut the cord and removed the transverse bar. Then he\n",
      "tried the various keys in the lock, but without success. No sound came\n",
      "from within, and at the silence Holmes‚Äô face clouded over. ‚ÄúI trust that we are not too late,‚Äù said he. ‚ÄúI think, Mis...\n",
      "\n",
      "[5] Sherlock Holmes sat moodily at one side of the\n",
      "fireplace cross-indexing his records of crime, while I at the other was\n",
      "deep in one of Clark Russell‚Äôs fine sea-stories until the howl of the\n",
      "gale from without seemed to blend with the text, and the s...\n",
      "\n",
      "==================================================\n",
      "Here are moments from the passages where Holmes or Watson reflect on friendship:\n",
      "\n",
      "1. When Holmes wakes Watson early in the morning about a new case, he says:\n",
      "> ‚ÄúMy name is Sherlock Holmes. This is my intimate friend and associate, Dr. Watson, before whom you can speak as freely as before myself.\"\n",
      "\n",
      "This shows Holmes introducing Watson as his \"intimate friend and associate,\" reflecting on their close friendship and trust.\n",
      "\n",
      "2. Watson reflects on his pleasure in accompanying Holmes:\n",
      "> I had no keener pleasure than in following Holmes in his professional investigations, and in admiring the rapid deductions, as swift as intuitions, and yet always founded on a logical basis with which he unravelled the problems which were submitted to him.\n",
      "\n",
      "This is a direct reflection by Watson on his enjoyment and appreciation of his friendship and partnership with Holmes.\n",
      "\n",
      "3. Holmes, when asked if the late-night visitor is a friend, replies:\n",
      "> ‚ÄúWhy,‚Äù said I, glancing up at my companion, ‚Äúthat was surely the bell. Who could come to-night? Some friend of yours, perhaps?‚Äù\n",
      ">\n",
      "> ‚ÄúExcept yourself I have none,‚Äù he answered. ‚ÄúI do not encourage visitors.‚Äù\n",
      "\n",
      "Here, Holmes states that Watson is his only friend, which is a direct reflection on their friendship.\n",
      "\n",
      "These passages show both Holmes and Watson reflecting on their friendship and the closeness of their relationship.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_response(\"Show moments where Holmes or Watson reflect on friendship.\", ranking=\"cross_encoder\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ac6e5-a9b3-43e4-95ba-a9a5a0cf2cc3",
   "metadata": {},
   "source": [
    "**Reranking with Late Interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a43e2986-2222-400e-b0b6-ff1f952af641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided Chunks (top 5, ranking: late_interaction):\n",
      "------------------------------------------------------------\n",
      "[1] This gentleman, Mr. Wilson, has been my partner and helper\n",
      "in many of my most successful cases, and I have no doubt that he will\n",
      "be of the utmost use to me in yours also.‚Äù\n",
      "\n",
      "The stout gentleman half rose from his chair and gave a bob of\n",
      "greeting,...\n",
      "\n",
      "[2] Holmes unlocked his strong-box and held up the blue\n",
      "carbuncle, which shone out like a star, with a cold, brilliant,\n",
      "many-pointed radiance. Ryder stood glaring with a drawn face, uncertain\n",
      "whether to claim or to disown it. ‚ÄúThe game‚Äôs up, Ryder,‚Äù s...\n",
      "\n",
      "[3] THE ADVENTURE OF THE ENGINEER‚ÄôS THUMB\n",
      "\n",
      "\n",
      "Of all the problems which have been submitted to my friend, Mr.\n",
      "Sherlock Holmes, for solution during the years of our intimacy, there\n",
      "were only two which I was the means of introducing to his notice‚Äîthat\n",
      "...\n",
      "\n",
      "[4] Holmes cut the cord and removed the transverse bar. Then he\n",
      "tried the various keys in the lock, but without success. No sound came\n",
      "from within, and at the silence Holmes‚Äô face clouded over. ‚ÄúI trust that we are not too late,‚Äù said he. ‚ÄúI think, Mis...\n",
      "\n",
      "[5] A few moments later he was in our room, still puffing, still\n",
      "gesticulating, but with so fixed a look of grief and despair in his\n",
      "eyes that our smiles were turned in an instant to horror and pity. For\n",
      "a while he could not get his words out, but swa...\n",
      "\n",
      "==================================================\n",
      "Here are moments where Holmes or Watson reflect on friendship, as shown in the provided passages:\n",
      "\n",
      "1. Holmes refers to Watson as his friend and partner, acknowledging his importance and usefulness in cases:\n",
      "> This gentleman, Mr. Wilson, has been my partner and helper in many of my most successful cases, and I have no doubt that he will be of the utmost use to me in yours also.\n",
      "\n",
      "2. Watson reflects on his relationship with Holmes, mentioning their intimacy and his role in introducing cases:\n",
      "> Of all the problems which have been submitted to my friend, Mr. Sherlock Holmes, for solution during the years of our intimacy, there were only two which I was the means of introducing to his notice‚Äîthat of Mr. Hatherley‚Äôs thumb, and that of Colonel Warburton‚Äôs madness.\n",
      "\n",
      "3. Watson also describes his continued visits to Holmes after marriage, showing their ongoing friendship:\n",
      "> I had returned to civil practice and had finally abandoned Holmes in his Baker Street rooms, although I continually visited him and occasionally even persuaded him to forgo his Bohemian habits so far as to come and visit us.\n",
      "\n",
      "These passages demonstrate moments where Holmes and Watson reflect on their partnership and friendship.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_response(\"Show moments where Holmes or Watson reflect on friendship.\", ranking=\"late_interaction\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2d47b-9532-464d-97e8-212c68c8c190",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "In the above notebook we've demonstrated a two step retrieval and reranking process covering the tradition bi-encoder architecture for simple retrieval then cross-encoder and late interaction style setups and models for computing. The workings and findings of such are outlined in the below summary table:\n",
    "\n",
    "**Comparison Table**\n",
    "\n",
    "| Step         | Retrieval (Embedding)          | Rerank: Late Interaction (ColBERT)   | Rerank: Cross-Encoder           |\n",
    "| ------------ | ------------------------------ | ------------------------------------ | ------------------------------- |\n",
    "| Query Encode | 1 vector per query             | Matrix of vectors per query          | Joint encoding (query+doc pair) |\n",
    "| Doc Encode   | 1 vector per doc (pre-compute) | Matrix of vectors per doc (pre-comp) | N/A (encode per query+doc pair) |\n",
    "| Scoring      | Cosine/dot similarity          | MaxSim per query token + aggregate   | Full transformer, \\[CLS] output |\n",
    "| Compute cost | Very low                       | Moderate (matrix op per pair)        | High (full forward per pair)    |\n",
    "\n",
    "In essence, the bi-encoder is a recall-oriented coarse retrieval tool, while the cross-encoder and late interaction re-ranker is a precision-oriented fine reranking step. By relying on the traditional bi-encoder to quickly create a subset that can then be refined by a more compute intensive reranking model and improve retrieval results and downstream LLM generated contextual responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
